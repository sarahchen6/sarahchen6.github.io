<!DOCTYPE html>
<!-- saved from url=(0139)file:///Users/esther-cao/Downloads/What%E2%80%99s%20in%20a%20Question_%20Using%20Visual%20Questions%20as%20a%20Form%20of%20Supervision.html -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">


 
  <meta name="google-site-verification" content="bnoFBnIUa_ExNW78iL9iiIrJdu4iq-eO1TFCVn_mBQ4">
   <meta name="author" content="Sarah Chen, Esther Cao">
<meta name="keywords" content="sarah chen,esther cao,cmu,carnegie mellon university,swath gaps,Machine Learning,swath gaps,unsupervised machine learning,NASA MODIS instruments">
  <meta name="description" content="Reducing Effects of Swath Gaps on Unsupervised Machine Learning Models for NASA MODIS Instruments">

<meta name="title" content="Reducing Effects of Swath Gaps on Unsupervised Machine Learning Models for NASA MODIS Instruments">



  <title>Reducing Effects of Swath Gaps on Unsupervised Machine Learning Models for NASA MODIS Instruments</title>

  <!-- bootstrap -->
  <link rel="stylesheet" href="./bootstrap.min.css">
  <link rel="stylesheet" href="./bootstrap-theme.min.css">

  <!-- Google fonts -->
  <link href="./css" rel="stylesheet" type="text/css">

  <!-- Google Analytics -->
  <link rel="stylesheet" type="text/css" href="./style.css">

  <script async="" src="./analytics.js"></script><script async="" src="./analytics(1).js"></script><script async="" src="./analytics(2).js"></script><script>
  function page_loaded() {
  }
  </script>
</head>

<body onload="page_loaded()">

<div id="header">
  <a href="http://cvpr2017.thecvf.com/">
    <img src="./space_ml_logo.png" style="height:60px; float: left; margin-left: 20px;">
  </a>
  
  <a href="http://www.cmu.edu/">
    <img src="./1024px-Carnegie_Mellon_University_seal.svg.png" style="height:60px; float: right; margin-right: 60px;">
  </a>
  <h1><b>Reducing Effects of Swath Gaps on Unsupervised Machine Learning Models<\b><br>for NASA MODIS Instruments</h1>
  <div style="clear:both;"></div>
</div>

<!--
<div id="teaser">
</div>
-->

<div class="sechighlight">
<div class="container sec">
  <h2>Abstract</h2>

  <div id="coursedesc">
    Due to the nature of their pathways, NASA Terra and NASA Aqua satellites capture imagery containing “swath gaps'' which are areas of no data. Swath gaps can overlap the region of interest (ROI) completely, often rendering the entire imagery unusable by Machine Learning (ML) models. This problem is further exacerbated when the ROI rarely occurs (e.g. a hurricane) and, on occurrence, is partially overlapped with a swath gap. With annotated data as supervision, a model can learn to differentiate between the area of focus and the swath gap. However, annotation is expensive and currently the vast majority of existing data is unannotated. Hence, we propose an augmentation technique that considerably removes the existence of swath gaps in order to allow CNNs to focus on the ROI, and thus successfully use data with swath gaps for training. We experiment on the UC Merced Land Use Dataset, where we add swath gaps through empty polygons (up to 20% areas) and then apply augmentation techniques to fill the swath gaps. We compare the model trained with our augmentation techniques on the swath gap-filled data with the model trained on the original swath gap-less data and note highly augmented performance. Additionally, we perform a qualitative analysis using activation maps that visualizes the effectiveness of our trained network in not paying attention to the swath gaps. We also evaluate our results with a human baseline and show that, in certain cases, the filled swath gaps look so realistic that even a human evaluator did not distinguish between original satellite images and swath gap-filled images. Since this method is aimed at unlabeled data, it is widely generalizable and impactful for large scale unannotated datasets from various space data domains. 
  </div>
</div>
</div>

<div class="container sec" align="center">

<!-- <a href="../densecap.pdf" style="font-size:22px">Paper</a><br> -->

<a href="https://arxiv.org/pdf/1704.03895.pdf">
  <img src="./essay_thumbnail.png" style="width:100%;border: 1px solid #AAA;">
</a>

<br>
<div align="center">
  <div class="instructor" align="center">
    <a href="http://sidgan.me/siddhaganju">
    <div class="instructorphoto"><img src="./What’s in a Question_ Using Visual Questions as a Form of Supervision_files/siddha.jpg"></div>
    <div>Siddha Ganju</div>
    </a>
  </div>
  <div class="instructor" align="center">
    <a href="http://www.cs.cmu.edu/~orussako/index.html">
    <div class="instructorphoto"><img src="./What’s in a Question_ Using Visual Questions as a Form of Supervision_files/OlgaRussakovsky_highres.JPG"></div>
    <div>Olga Russakovsky</div>
    </a>
  </div>
</div>
<div style="color:#900;" align="center">
  <br>
 <p> Spotlight, CVPR 2017  </p>
 <p>    <a href="http://languageandvision.com/">Spotlight, Language and Vision workshop</a> </p>
  <p>   <a href="http://www.visualqa.org/abstracts.html"> Poster, VQA workshop</a>
</p>


</div>

</div>

<div class="sechighlight">
<div class="container sec" style="font-size:18px">
  <div class="row">

    <div class="col-md-5">
      <h2>Links</h2>
      <ul>
        <li> <a href="https://arxiv.org/pdf/1704.03895.pdf">Paper</a> </li>
        <li> <a href="https://arxiv.org/pdf/1704.03895.pdf">Supplementary (attached as Appendix)</a> </li>
        <li> <a href="https://www.slideshare.net/szeusg/whats-in-a-question-using-visual-questions-as-a-form-of-supervision">CVPR'17 Presentation</a> </li>
        <li> <a href="https://www.youtube.com/watch?v=RzdPkZHv62U">CVPR'17 Official Video (from CVF)</a> </li>
        <li> <a href="https://github.com/sidgan/sidgan.github.com/raw/master/images/cvpr17_poster_ganju_russ_gupta.pdf">CVPR'17 Poster</a> </li>
        <li> <a href="https://www.youtube.com/watch?v=CVuaaRmiCGE&amp;feature=youtu.be">CVPR'17 Video (Better audio)</a> </li>
        <li> <a href="https://github.com/sidgan/whats_in_a_question">Github</a> </li>
        <li>  <a href="https://github.com/sidgan/whats_in_a_question/blob/master/vqa/README.md#files">Pretrained models</a>    </li>
        <li>  <a href="https://github.com/sidgan/whats_in_a_question/blob/master/vqa/README.md#files">Preprocessed data</a>    </li>
      </ul>
    </div>
    <div class="col-md-7">
      <h2>Bibtex</h2>
<pre style="font-size:12px;">@inproceedings{GanjuCVPR17,
author = {Siddha Ganju and Olga Russakovsky and Abhinav Gupta},
title = {What's in a Question: Using Visual Questions as a Form of Supervision},
booktitle = {CVPR},
year = {2017}
}
</pre>
    </div>

  </div>
</div>
</div>

<div class="container sec">


  <h2></h2>

There are three tasks described in the paper: 


<h2>1. Image Descriptions </h2>
We analyze whether the visual questions contain enough information to provide an accurate description of the image using the Seq2Seq model.


 <div id="gallery" align="center">
    <div class="egimg" align="center">
      <img src="./What’s in a Question_ Using Visual Questions as a Form of Supervision_files/fig2.png">
    </div>
  </div>

Three visual questions and the caption generated from them using the Seq2Seq model. Some captions are surprisingly accurate (green) while others less so (orange).




<h2>2. Object Classification</h2>
Visual questions can provide information about the object classes that are present in the image. E.g., asking “what color is the bus?” indicates the presence of a bus in the image.




 <div id="gallery" align="center">
    <div class="egimg">
      <img src="./What’s in a Question_ Using Visual Questions as a Form of Supervision_files/object_classification.png">
    </div>
  </div>


<h2>3. Visual Question Answering</h2>
Visual Question Answering is, given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Visual questions focus on different areas of an image, including background details and underlying context. We utilize not just the target question, but also the unanswered questions about a particular image. 


 

  <div id="gallery" align="center">
    <div class="egimg" align="center">
      <img src="./What’s in a Question_ Using Visual Questions as a Form of Supervision_files/fig1.png">
    </div>
  </div>

   Qualitative comparison of our iBOWIMG-2x <i>(left)</i> and the baseline iBOWIMG <i>(right)</i> results. Correct answers in green; wrong answers in red.

















</div>


<div class="sechighlight">
<div id="footer">
  
<br>
<br>

We would like to thank Anirudh Koul, Siddha Ganju, Meher Anand Kasam, and Satyarth Praveen for helpful discussions. 
<br>
<br>

Website code reference: 
<a href="http://cs.stanford.edu/people/karpathy/densecap/">DenseCap: Fully Convolutional Localization Networks for Dense Captioning</a>
<br>
<br>

Thank you Salvador Medina for helping generate the <a href="https://github.com/salmedina/pdf2thumb">pretty pdf.</a>


</div>
</div>


<!-- jQuery and Boostrap -->
<script src="./jquery.min.js"></script>
<script src="./bootstrap.min.js"></script>

<!-- Analytics -->

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-59744528-2', 'auto');
  ga('send', 'pageview');

</script>





</body></html>
